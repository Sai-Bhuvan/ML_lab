import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score

# Load and preprocess data
df = pd.read_csv('diabetes.csv')
df.fillna(df.mean(), inplace=True)  # Replace missing values with mean

# Normalize data
scaler = MinMaxScaler()
features = ['Glucose', 'BloodPressure', 'Insulin', 'BMI', 'Age']  # example features
df[features] = scaler.fit_transform(df[features])

# Split dataset
X = df[features]
y = df['Outcome']  # Assuming 'Outcome' is the target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define Euclidean distance function
def euclidean_distance(row1, row2):
    return np.sqrt(np.sum((row1 - row2) ** 2))

# Get K nearest neighbors
def get_neighbors(X_train, y_train, test_row, num_neighbors):
    distances = []
    for i in range(len(X_train)):
        dist = euclidean_distance(test_row, X_train.iloc[i])
        distances.append((y_train.iloc[i], dist))
    distances.sort(key=lambda x: x[1])
    neighbors = [distances[i][0] for i in range(num_neighbors)]
    return neighbors

# Predict the class
def predict_classification(X_train, y_train, test_row, num_neighbors):
    neighbors = get_neighbors(X_train, y_train, test_row, num_neighbors)
    prediction = max(set(neighbors), key=neighbors.count)
    return prediction

# Test the KNN model
y_pred = [predict_classification(X_train, y_train, row, num_neighbors=5) for index, row in X_test.iterrows()]
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of the KNN model:", accuracy)
